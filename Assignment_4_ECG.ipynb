{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djQINsl_Ab3V"
   },
   "source": [
    "# Use Autoencoder to implement anomaly detection. Build the model by using:\n",
    "a. Import required libraries<br>\n",
    "b. Upload / access the dataset<br>\n",
    "c. Encoder converts it into latent representation<br>\n",
    "d. Decoder networks convert it back to the original input<br>\n",
    "e. Compile the models with Optimizer, Loss, and Evaluation Metrics<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1698430145668,
     "user": {
      "displayName": "Ranjeet Kumbhar",
      "userId": "11440510226724121904"
     },
     "user_tz": -330
    },
    "id": "HnMj8dfZDIWx"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the path to the dataset. You can change this to your local file path if needed.\n",
    "path = 'http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv'\n",
    "\n",
    "# Read the ECG dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(path, header=None)\n",
    "\n",
    "\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get information about the dataset, such as column data types and non-null counts\n",
    "data.info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the dataset into features and target\n",
    "features = data.drop(140, axis=1)  # Features are all columns except the last (column 140)\n",
    "target = data[140]  # Target is the last column (column 140)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2\n",
    ")\n",
    "\n",
    "# Get the indices of the training data points labeled as \"1\" (anomalies)\n",
    "train_index = y_train[y_train == 1].index\n",
    "\n",
    "# Select the training data points that are anomalies\n",
    "train_data = x_train.loc[train_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Min-Max Scaler to scale the data between 0 and 1\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the training data\n",
    "x_train_scaled = min_max_scaler.fit_transform(train_data.copy())\n",
    "\n",
    "# Scale the testing data using the same scaler\n",
    "x_test_scaled = min_max_scaler.transform(x_test.copy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating an Autoencoder model by extending the Model class from Keras\n",
    "class AutoEncoder(Model):\n",
    "    def __init__(self, output_units, ldim=8):\n",
    "        super().__init__()\n",
    "        # Define the encoder part of the Autoencoder\n",
    "        self.encoder = Sequential([\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(ldim, activation='relu')\n",
    "        ])\n",
    "        # Define the decoder part of the Autoencoder\n",
    "        self.decoder = Sequential([\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(output_units, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass through the Autoencoder\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the AutoEncoder model with the appropriate output units\n",
    "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
    "\n",
    "# Compile the model with Mean Squared Logarithmic Error (MSLE) loss and Mean Squared Error (MSE) metric\n",
    "model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
    "\n",
    "# Train the model using the scaled training data\n",
    "history = model.fit(\n",
    "    x_train_scaled,  # Input data for training\n",
    "    x_train_scaled,  # Target data for training (autoencoder reconstructs the input)\n",
    "    epochs=20,        # Number of training epochs\n",
    "    batch_size=512,   # Batch size\n",
    "    validation_data=(x_test_scaled, x_test_scaled),  # Validation data\n",
    "    shuffle=True     # Shuffle the data during training\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to find the threshold for anomalies based on the training data\n",
    "def find_threshold(model, x_train_scaled):\n",
    "    # Reconstruct the data using the model\n",
    "    recons = model.predict(x_train_scaled)\n",
    "\n",
    "    # Calculate the mean squared log error between reconstructed data and the original data\n",
    "    recons_error = tf.keras.metrics.msle(recons, x_train_scaled)\n",
    "\n",
    "    # Set the threshold as the mean error plus one standard deviation\n",
    "    threshold = np.mean(recons_error.numpy()) + np.std(recons_error.numpy())\n",
    "\n",
    "    return threshold\n",
    "\n",
    "# Function to make predictions for anomalies based on the threshold\n",
    "def get_predictions(model, x_test_scaled, threshold):\n",
    "    # Reconstruct the data using the model\n",
    "    predictions = model.predict(x_test_scaled)\n",
    "\n",
    "    # Calculate the mean squared log error between reconstructed data and the original data\n",
    "    errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
    "\n",
    "    # Create a mask for anomalies based on the threshold\n",
    "    anomaly_mask = pd.Series(errors) > threshold\n",
    "\n",
    "    # Map True (anomalies) to 0 and False (normal data) to 1\n",
    "    preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
    "\n",
    "    return preds\n",
    "\n",
    "# Find the threshold for anomalies\n",
    "threshold = find_threshold(model, x_train_scaled)\n",
    "print(f\"Threshold: {threshold}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get predictions for anomalies based on the model and threshold\n",
    "predictions = get_predictions(model, x_test_scaled, threshold)\n",
    "\n",
    "# Calculate the accuracy score by comparing the predicted anomalies to the true labels\n",
    "accuracy = accuracy_score(predictions, y_test)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(f\"Accuracy Score: {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
